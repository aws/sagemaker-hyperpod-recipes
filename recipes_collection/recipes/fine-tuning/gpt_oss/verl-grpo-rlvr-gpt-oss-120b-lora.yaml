defaults:
  - /hydra_config/verl/default
  - /hydra_config/verl/algorithm/grpo
  - /hydra_config/verl/actor/default
  - /hydra_config/verl/ref/default
  - /hydra_config/verl/critic/default
  - /hydra_config/verl/rollout/vllm
  - /hydra_config/verl/actor_rollout_ref/default
  - /hydra_config/verl/trainer/default
  - /hydra_config/verl/data/default
  - /hydra_config/verl/reward_model/default
  - /hydra_config/verl/ray_cluster/default
  - /hydra_config/verl/peft/lora
  - /hydra_config/verl/reward/rlvr
  - /hydra_config/verl/model_config/gpt-oss-120b
  - _self_

display_name: GPT OSS 120B GRPO RLVR Fine-Tuning with LoRA
version: 1.1.0
instance_types:
- ml.p5e.48xlarge
- ml.p5en.48xlarge
run:
  name: verl-grpo-gpt-oss-120b-lora
trainer:
  num_nodes: 1
training_config:
  actor_rollout_ref:
    actor:
      ppo_micro_batch_size_per_gpu: null
      ppo_max_token_len_per_gpu: 32768
      entropy_checkpointing: true
      optim:
        lr: 1.0e-05
    ref:
      log_prob_micro_batch_size_per_gpu: null
      log_prob_max_token_len_per_gpu: 32768
      entropy_from_logits_with_chunking: true
    rollout:
      gpu_memory_utilization: 0.5
      log_prob_micro_batch_size_per_gpu: null
      log_prob_max_token_len_per_gpu: 32768
      enable_chunked_prefill: true
      layered_summon: false
      merge_lora_for_inference: true
    model:
      path: openai/gpt-oss-120b-bf16
      attn_implementation: kernels-community/vllm-flash-attn3
      is_saving_checkpoint_in_lora: false
      target_parameters:
      - mlp.experts.gate_up_proj
      - mlp.experts.down_proj
  trainer:
    project_name: grpo_example_gsm8k
    experiment_name: gpt-oss_120b_grpo_lora
    resume_mode: resume_path
    resume_from_path: ''
  data:
    train_batch_size: 1024
    val_batch_size: null
    shuffle: true
    chat_template_kwargs:
      reasoning_effort: medium
  critic:
    model:
      tokenizer_path: openai/gpt-oss-120b-bf16
  reward_model:
    model:
      input_tokenizer: openai/gpt-oss-120b-bf16
      path: sfairX/FsfairX-LLaMA3-RM-v0.1
    launch_reward_fn_async: false
  custom_reward_function:
    lambda_arn: ''
    path: ''
    name: ''
ray_cluster:
  worker_nodes:
    replicas: 1
