defaults:
  - /hydra_config/verl/default
  - /hydra_config/verl/algorithm/grpo
  - /hydra_config/verl/actor/default
  - /hydra_config/verl/ref/default
  - /hydra_config/verl/critic/default
  - /hydra_config/verl/rollout/vllm
  - /hydra_config/verl/actor_rollout_ref/default
  - /hydra_config/verl/trainer/default
  - /hydra_config/verl/data/default
  - /hydra_config/verl/reward_model/default
  - /hydra_config/verl/ray_cluster/default
  - /hydra_config/verl/peft/fft
  - /hydra_config/verl/reward/rlvr
  - /hydra_config/verl/model_config/gpt-oss-20b
  - _self_

display_name: "GPT OSS 20B GRPO RLVR Fine-Tuning"
version: "1.2.0"
instance_types: ["ml.p5.48xlarge"]

run:
  name: verl-grpo-gpt-oss-20b-fft

trainer:
  num_nodes: 2

training_config:
  actor_rollout_ref:
    actor:
      _target_: verl.workers.config.FSDPActorConfig
      strategy: fsdp2
      ppo_mini_batch_size: 32
      ppo_micro_batch_size: null
      ppo_micro_batch_size_per_gpu: 2
      use_dynamic_bsz: true
      ppo_max_token_len_per_gpu: 12288
      clip_ratio: 0.2
      clip_ratio_low: 0.2
      clip_ratio_high: 0.2
      policy_loss:
        _target_: verl.workers.config.PolicyLossConfig
        loss_mode: vanilla
        clip_cov_ratio: 0.0002
        clip_cov_lb: 1.0
        clip_cov_ub: 5.0
        kl_cov_ratio: 0.0002
        ppo_kl_coef: 0.1
      clip_ratio_c: 3.0
      loss_agg_mode: token-mean
      entropy_coeff: 0
      entropy_advantage: false
      entropy_advantage_alpha: 0.4
      entropy_advantage_kappa: 2
      use_kl_loss: true
      use_torch_compile: true
      kl_loss_coef: 0.001
      kl_loss_type: low_var_kl
      ppo_epochs: 1
      shuffle: false
      checkpoint:
        _target_: verl.trainer.config.CheckpointConfig
        save_contents:
        - hf_model
        load_contents:
        - hf_model
        async_save: false
      optim:
        lr: 1.0e-06
        lr_warmup_steps_ratio: 0.0
        total_training_steps: -1
        weight_decay: 0.01
        lr_warmup_steps: -1
        _target_: verl.workers.config.FSDPOptimizerConfig
        min_lr_ratio: 0.0
        num_cycles: 0.5
        warmup_style: constant
      use_fused_kernels: false
      grad_clip: 1.0
      ulysses_sequence_parallel_size: 1
      entropy_from_logits_with_chunking: false
      entropy_checkpointing: false
      fsdp_config:
        _target_: verl.workers.config.FSDPEngineConfig
        wrap_policy:
          min_num_params: 0
        param_offload: false
        optimizer_offload: false
        offload_policy: false
        reshard_after_forward: true
        fsdp_size: -1
        forward_prefetch: false
        model_dtype: bfloat16
      use_remove_padding: true
    ref:
      strategy: fsdp2
      use_torch_compile: true
      log_prob_micro_batch_size: null
      log_prob_micro_batch_size_per_gpu: 16
      log_prob_use_dynamic_bsz: true
      log_prob_max_token_len_per_gpu: 12288
      fsdp_config:
        _target_: verl.workers.config.FSDPEngineConfig
        wrap_policy:
          min_num_params: 0
        param_offload: true
        reshard_after_forward: true
        forward_prefetch: false
        model_dtype: bfloat16
      ulysses_sequence_parallel_size: 1
      entropy_from_logits_with_chunking: false
      entropy_checkpointing: false
    rollout:
      tensor_model_parallel_size: 2
      log_prob_micro_batch_size_per_gpu: 16
      log_prob_use_dynamic_bsz: true
      log_prob_max_token_len_per_gpu: 12288
      disable_log_stats: true
      do_sample: true
      'n': 8
      multi_stage_wake_up: false
      engine_kwargs:
        vllm:
          swap_space: null
          disable_mm_preprocessor_cache: false
        sglang:
          attention_backend: null
      val_kwargs:
        top_k: -1
        top_p: 1.0
        temperature: 0
        'n': 1
        do_sample: false
      multi_turn:
        enable: false
        max_assistant_turns: null
        tool_config_path: null
        max_user_turns: null
        max_parallel_calls: 1
        max_tool_response_length: 256
        tool_response_truncate_side: middle
        interaction_config_path: null
        use_inference_chat_template: false
        tokenization_sanity_check_mode: strict
        format: hermes
      calculate_log_probs: false
      agent:
        num_workers: 8
        agent_loop_config_path: null
        custom_async_server:
          path: null
          name: null
      update_weights_bucket_megabytes: 512
      trace:
        backend: mlflow
        token2text: false
      enable_chunked_prefill: true
      layered_summon: false
      merge_lora_for_inference: true
    model:
      path: openai/gpt-oss-20b-bf16
      attn_implementation: kernels-community/vllm-flash-attn3
      target_parameters:
      - mlp.experts.gate_up_proj
      - mlp.experts.down_proj
  trainer:
    experiment_name: gpt-oss-20b-fft_grpo
    nnodes: 2
  data:
    val_batch_size: null
    return_raw_input_ids: false
    return_raw_chat: false
    return_full_prompt: false
    shuffle: false
    dataloader_num_workers: 8
    validation_shuffle: false
    chat_template_kwargs:
      reasoning_effort: medium
    filter_overlong_prompts: true
    filter_overlong_prompts_workers: 1
    truncation: error
    image_key: images
    video_key: videos
    trust_remote_code: false
    custom_cls:
      path: null
      name: null
    return_multi_modal_inputs: true
    sampler:
      class_path: null
      class_name: null
    datagen:
      path: null
      name: null
  critic:
    model:
      tokenizer_path: openai/gpt-oss-20b-bf16
      override_config: {}
      external_lib: null
      trust_remote_code: false
      _target_: verl.workers.config.FSDPCriticModelCfg
      use_shm: false
      enable_gradient_checkpointing: true
      enable_activation_offload: false
      use_remove_padding: false
      fsdp_config:
        _target_: verl.workers.config.FSDPEngineConfig
        param_offload: false
        optimizer_offload: false
        offload_policy: false
        reshard_after_forward: true
        wrap_policy:
          min_num_params: 0
        fsdp_size: -1
        forward_prefetch: false
      lora_rank: 0
      lora_alpha: 16
      target_modules: all-linear
    ppo_mini_batch_size: 256
    ppo_micro_batch_size: null
    ppo_micro_batch_size_per_gpu: null
    use_dynamic_bsz: true
    ppo_max_token_len_per_gpu: 12288
    forward_max_token_len_per_gpu: 32768
    ppo_epochs: 1
    shuffle: false
    cliprange_value: 0.5
    loss_agg_mode: token-mean
    checkpoint:
      _target_: verl.trainer.config.CheckpointConfig
      save_contents:
      - hf_model
      load_contents:
      - hf_model
      async_save: false
    profiler:
      _target_: verl.utils.profiler.ProfilerConfig
      discrete: false
      all_ranks: false
      ranks: []
    forward_micro_batch_size: null
    forward_micro_batch_size_per_gpu: null
    ulysses_sequence_parallel_size: 1
    grad_clip: 1.0
  reward_model:
    model:
      input_tokenizer: openai/gpt-oss-20b-bf16
  custom_reward_function:
    lambda_arn: ''
    path: ''
    name: ''
