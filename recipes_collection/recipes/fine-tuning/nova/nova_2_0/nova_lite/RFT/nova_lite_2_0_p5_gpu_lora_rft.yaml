## Nova Lite RLVR Training (PEFT)
## Run config - job metadata and replica counts
run:
  name: my-rft-run
  model_type: amazon.nova-2-lite-v1:0:256k
  model_name_or_path: nova-lite-2/prod
  data_s3_path : s3://example-bucket/train.jsonl
  output_s3_path: ""

  replicas: 2 # Number of compute instances for training. All supported values: {2, 4, 8, 16}
  generation_replicas: 2 # LLM inference replicas
  rollout_worker_replicas: 1

  # Lambda functions for RFT
  reward_lambda_arn: "" # Add your reward lambda arn here
  mlflow_tracking_uri: "" # Required for MLFlow
  mlflow_experiment_name: "my-rft-experiment" # Optional for MLFlow. Note: leave this field non-empty
  mlflow_run_name: "my-rft-run" # Optional for MLFlow. Note: leave this field non-empty

## Training config
training_config:
  max_length: 10240
  global_batch_size: 256
  reasoning_effort: high

  data:
    shuffle: false

  rollout:
    rollout_strategy:
      type: off_policy_async
      age_tolerance: 2
    advantage_strategy:
      number_generation: 8
    generator:
      max_new_tokens: 8192
      set_random_seed: true
      temperature: 1
      top_k: 0
    rewards:
      api_endpoint:
        lambda_arn: ${oc.select:run.reward_lambda_arn}
        lambda_concurrency_limit: 100 # Lambda should be able to handle (rollout_worker_replicas * 64) requests

  # Training configuration
  trainer:
    max_steps: 100
    save_steps: ${oc.select:training_config.trainer.max_steps}
    save_top_k: 5

    # RL parameters
    refit_freq: 4
    clip_ratio_high: 0.2
    ent_coeff: 0.001
    loss_scale: 1

    optim_config:                    # Optimizer settings
      lr: 7e-7                       # Learning rate
      weight_decay: 0.0              # L2 regularization strength (0.0â€“1.0)
      adam_beta1: 0.9
      adam_beta2: 0.95

    # For Peft recipe - LoRA settings
    peft:
      peft_scheme: "lora" # Enable LoRA for parameter-efficient fine-tuning
      lora_tuning:
        alpha: 32 # Scaling factor for LoRA weights. Allowed values are 32, 64, 96, 128, 160 and 192
        loraplus_lr_ratio: 64.0 # LoRA+ learning rate scaling factor, must be between 0.0 and 100.0
