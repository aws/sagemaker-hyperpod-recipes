# Note:
# This recipe can run on p5.48xlarge and p5en.48xlarge instance types.

## Run config
run:
  name: "my-rft-run"                         # Unique run name (appears in logs/artifacts).
  model_type: amazon.nova-2-lite-v1:0:256k
  model_name_or_path: nova-lite-2/prod
  data_s3_path: s3://<bucket>/<data file>    # Training dataset in JSONL;
  replicas: 4
  reward_lambda_arn: arn:aws:lambda:<region>:<account-id>:function:<function-name>

## SMTJ GRPO Training specific configs
training_config:
  max_length: 8192                         # Context window (tokens) for inputs+prompt;
  global_batch_size: 64                     # Total samples per optimizer step across all replicas (16/32/64/128/256).
  reasoning_effort: high                    # Enables reasoning mode High / Low / or null for non-reasoning

  rollout:                                  # How responses are generated for GRPO/advantage calc.
    advantage_strategy:
      number_generation: 2                  # N samples per prompt to estimate advantages (variance vs cost).
    generator:
      max_new_tokens: 6000                 # Cap on tokens generated per sample
      set_random_seed: true                 # Seed generation for reproducibility across runs.
      temperature: 1                        # Softmax temperature;
      top_k: 1                              # Sample only from top-K logits
    rewards:
      api_endpoint:
        lambda_arn: arn:aws:lambda:<region>:<account-id>:function:<function-name>
        lambda_concurrency_limit: 12        # Max concurrent Lambda invocations (throughput vs. throttling).

  trainer:
    max_steps: 5                            # Steps to train for. One Step = global_batch_size
    save_steps: ${oc.select:training_config.trainer.max_steps}

    # RL parameters
    ent_coeff: 0.0                          # A bonus added to the policy loss that rewards higher-output entropy.
    kl_loss_coef: 0.001                     # Weight on the KL penalty between the actor (trainable policy) and a frozen reference model

    optim_config:                           # Optimizer settings
      lr: 1e-4                              # Learning rate
      weight_decay: 0.0                     # L2 regularization strength (0.0–1.0)
      adam_beta1: 0.9
      adam_beta2: 0.95

    peft:                                   # Parameter-efficient fine-tuning (LoRA)
      peft_scheme: "lora"                   # Enable LoRA for PEFT
      lora_tuning:
        alpha: 32
        lora_plus_lr_ratio: 64.0            # LoRA+ learning rate scaling factor (0.0–100.0)
