## Run config
run:
    name: "my-fullrank-run"             # A descriptive name for your training job
    model_type: "amazon.nova-pro-v1:0:300k"  # Model variant specification, do not change
    model_name_or_path: "nova-pro/prod"      # Base model path, do not change
    replicas: 6                     # Number of compute instances for training. All supported values: {6, 12, 24}
    data_s3_path: ""                # Customer data path
    output_s3_path: ""              # Output artifact path, Sagemaker Hyperpod job-specific configuration - not compatible with standard Sagemaker Training jobs

## Training specific configs
training_config:
    max_length: 16384               # Maximum context window size (tokens). Should be between [1024, 32768] and multiple of 1024.
    global_batch_size: 32           # Total samples per step. Limits: {16, 32, 64, 128}

    trainer:
        max_epochs: 3               # Number of training epochs

    model:
        hidden_dropout: 0.0          # Dropout for hidden states. Limits: [0.0, 1.0]
        attention_dropout: 0.0       # Dropout for attention weights. Limits: [0.0, 1.0]
        ffn_dropout: 0.0             # Dropout for feed-forward networks. Limits: [0.0, 1.0]

        optim:
            lr: 1e-5                 # Learning rate
            name: distributed_fused_adam  # Optimizer algorithm, do not change
            adam_w_mode: true        # Enable AdamW mode
            eps: 1e-08               # Epsilon for numerical stability
            weight_decay: 0.01       # L2 regularization strength
            betas:                   # Adam optimizer betas. Limits: [0.0, 1.0]
                - 0.9
                - 0.999
            sched:
                warmup_steps: 10     # Learning rate warmup steps
                constant_steps: 0    # Steps at constant learning rate
                min_lr: 1e-6         # Minimum learning rate

        dpo_cfg:
            beta: 0.1               # Strength of preference enforcement. Limits: [0.001, 0.5]

        peft:
            peft_scheme: null        # Disable LoRA, trigger full rank fine tuning
