## Run config
run:
  name: "data-distillation-pro-teacher"     # A descriptive name for your training job

## Training specific configs
training_config:
  distillation_data: "true"                    # Enable data distillation job, do not change
  maxNumberOfPrompts: "20000"                  # Maximum number of prompts in the dataset
  maxResponseLength: "5000"                    # Maximum response length per prompt (tokens)
  minNumberOfPrompts: "0"                      # Minimum number of prompts required in the dataset
  maxInputFileSizeInGB: "2"                    # Maximum size of the input file (in GB)
  maxLineLengthInKB: "180"                     # Maximum size of a single line in the input file (in KB)

  # Maximum context window size (tokens) for student model. Must not exceed student model capacity.
  # You can set this value to 32k or 64k based on student model capacity.
  maxStudentModelFineTuningContextLengthInTokens: "32000"

  # Teacher model ARN. Do not modify unless you're using a region-specific variant.
  # For example, change to "eu.amazon.nova-pro-v1:0" if using the ARN region.
  teacherModelId: "us.amazon.nova-pro-v1:0"

  # Note: Adjust the following parameters based on the chosen student model
  temperature: "0.7"                           # Sampling temperature for generation. Higher = more random
  top_p: "0.9"                                 # Top-p nucleus sampling cutoff

  customer_bucket: ""  # S3 bucket for input/output data
  kms_key: ""  # AWS KMS key to encrypt input/output in S3
