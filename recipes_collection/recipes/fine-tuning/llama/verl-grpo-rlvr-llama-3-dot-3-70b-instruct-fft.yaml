defaults:
  - /hydra_config/verl/default
  - /hydra_config/verl/algorithm/grpo
  - /hydra_config/verl/actor/default
  - /hydra_config/verl/ref/default
  - /hydra_config/verl/critic/default
  - /hydra_config/verl/rollout/vllm
  - /hydra_config/verl/actor_rollout_ref/default
  - /hydra_config/verl/trainer/default
  - /hydra_config/verl/data/default
  - /hydra_config/verl/reward_model/default
  - /hydra_config/verl/ray_cluster/default
  - /hydra_config/verl/peft/fft
  - /hydra_config/verl/reward/rlvr
  - /hydra_config/verl/model_config/llama-3-3-70b-instruct
  - _self_

display_name: "Llama 3.3 70B Instruct GRPO RLVR Fine-Tuning"
version: "1.1.0"
instance_types: ["ml.p5.48xlarge"]

run:
  name: verl-grpo-llama-3-dot-3-70b-instruct-fft
trainer:
  num_nodes: 2

training_config:
  actor_rollout_ref:
    actor:
      ppo_max_token_len_per_gpu: 8192
      entropy_from_logits_with_chunking: true
      fsdp_config:
        param_offload: true
        optimizer_offload: true
    ref:
      log_prob_micro_batch_size_per_gpu: 16
      log_prob_max_token_len_per_gpu: 8192
      fsdp_config:
        param_offload: false
    rollout:
      prompt_length: 1024
      max_num_batched_tokens: 4096
      log_prob_micro_batch_size_per_gpu: 16
      log_prob_max_token_len_per_gpu: 8192
      merge_lora_for_inference: false
  trainer:
    experiment_name: llama-3.3-70b-instruct-fft_grpo-20251119.214144
    nnodes: 2
    test_freq: 7
  data:
    max_prompt_length: 1024
  critic:
    rollout_n: 6
    ppo_max_token_len_per_gpu: 32768
  reward_model:
    model:
      path: sfairX/FsfairX-LLaMA3-RM-v0.1
  custom_reward_function:
    lambda_arn: ''
    path: ''
    name: ''
