defaults:
  - /hydra_config/llmft/default
  - /hydra_config/llmft/model_config/llama-3-3-70b-instruct
  - /hydra_config/llmft/peft/fft
  - /hydra_config/llmft/algorithm/sft
  - /hydra_config/llmft/datasets/default_sft_fft
  - /hydra_config/llmft/strategy/fsdp_fft
  - _self_

display_name: "Llama 3.3 70B Instruct Supervised Fine-Tuning Full Fine-Tuning on GPU"
version: "1.2.0"
instance_types: ['ml.p4de.24xlarge','ml.p5.48xlarge']

run:
  name: llama-3-3-70b-instruct

trainer:
  num_nodes: 4

training_config:
  model_config:
    model_name_or_path: "meta-llama/Llama-3.3-70B-Instruct"
    model_save_name: SageMaker-FFT/Llama-3.3-70B-Instruct
    chat_template_key: llama--meta-llama-3.3-70b-instruct
  training_args:
    train_batch_size: 32

fsdp_config:
  state_dict_type: SHARDED_STATE_DICT
