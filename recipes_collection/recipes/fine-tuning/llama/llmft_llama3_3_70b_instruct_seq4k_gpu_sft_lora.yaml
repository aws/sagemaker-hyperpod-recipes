display_name: "Llama 3.3 70B Supervised Fine-Tuning with LoRA on GPU, 4K sequence length"
version: "1.0.0"
instance_types: ["ml.p4de.24xlarge","ml.p5.48xlarge"]

run:
  name: llama-3-3-70b-instruct
  model_type: llm_finetuning_aws
  results_dir: ${base_results_dir}/${.name}
  hf_access_token: null

trainer:
  devices: 8
  num_nodes: 1


training_config:
  mlflow:
    tracking_uri: ""
    run_id: ""
  force_rerun: false
  model_config:
    model_name_or_path: "meta-llama/Llama-3.3-70B-Instruct"
    lora_checkpoint_file_path: null
    multimodal: false
    peft_config:
      peft_type: lora
      target_modules: all-linear
      r: 16
      lora_alpha: 32
      lora_dropout: 0.05
      bias: none
      task_type: CAUSAL_LM
      use_dora: false
    model_save_name: SageMaker/llama-3.3-70B-Instruct-Lora
    chat_template_key: llama--meta-llama-3.3-70b-instruct
    apply_chat_template: false
    device_map: null
    attn_implementation: sdpa
    generation_cfg:
      max_new_tokens: 2048
      temperature: 0.6
      top_p: 0.9
      do_sample: true
      num_return_sequences: 1
      num_parallel_jobs: 1
  datasets:
    data_dir: null
    save_intermediate_processing_steps: false
    train_data:
      name: ""
      file_path: ""
      limit: null
    val_data:
      name: ""
      file_path: ""
      limit: null
    train_val_split_ratio: 0.9
    train_val_split_seed: 42
    preprocessor_cfgs:
    - type: TrainingSFTSampleConverter
    - type: Seq2SeqTokensProcessor
      chat_template_name: llama_3_2_seq_instruct.jinja
      ignore_prompt_labels: true
    - type: DropColumns
      column_names:
      - messages
  training_args:
    override_training_dir: true
    trainer_type: sft
    trainer_callbacks:
    - _target_: metering_callback.MeteringCallback
      output_path: "/opt/ml/metering"
    training_dir: ""
    seed: 42
    packing_samples: false
    ring_attn_size: 1
    ring_head_stride: 1
    pretrain_mode: false
    aux_loss_coef: 0
    learning_rate: 0.0001
    lr_warmup_ratio: 0.1
    weight_decay: 0.0
    adam_betas:
    - 0.9
    - 0.95
    lr_scheduler: cosine
    l2: 0.0
    gradient_clipping: true
    gradient_clipping_threshold: 1.0
    grad_accum_dtype: null
    gradient_checkpointing: true
    gradient_checkpointing_use_reentrant: false
    load_checkpoint: false
    resume_checkpoint_path: ""
    disable_ds_ckpt: true
    disable_fast_tokenizer: false
    micro_train_batch_size: 1
    train_batch_size: 8
    eval_steps: 0
    save_steps: 0
    save_hf_ckpt: true
    merge_weights: true
    max_epochs: 5
    max_len: 4096
    max_norm: 1.0
    max_samples: 100000000.0
    max_ckpt_mem: 100000000.0
    max_ckpt_num: 3
    logging_steps: 1
    use_tensorboard: logs
    use_wandb: null
    strategy:
      fsdp_config:
        auto_wrap_policy: TRANSFORMER_BASED_WRAP
        backward_prefetch: BACKWARD_PRE
        cpu_ram_efficient_loading: true
        cpu_offload: false
        forward_prefetch: false
        sharding_strategy: HYBRID_SHARD
        state_dict_type: SHARDED_STATE_DICT
        sync_module_states: true
        use_orig_params: true

##########################################################################################
###         The following configurations are used in elastic training only
##########################################################################################
elastic_policy:
  is_elastic: false
  min_nodes: 1
  max_nodes: 16
  use_graceful_shutdown: true
  scaling_timeout: 600
  graceful_shutdown_timeout: 900

scale_config:
  1:
    trainer:
      num_nodes: 1
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 4
        learning_rate: 0.0004
  2:
    trainer:
      num_nodes: 2
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 4
        learning_rate: 0.0004
  4:
    trainer:
      num_nodes: 4
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 4
        learning_rate: 0.0004
  6:
    trainer:
      num_nodes: 6
    training_config:
      training_args:
        train_batch_size: 128
        learning_rate: 0.0004
        uneven_batch:
          use_uneven_batch: true
          num_dp_groups_with_small_batch_size: 16
          small_local_batch_size: 2
          large_local_batch_size: 3
  8:
    trainer:
      num_nodes: 8
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 2
        learning_rate: 0.0004
  16:
    trainer:
      num_nodes: 16
    training_config:
      training_args:
        train_batch_size: 128
        learning_rate: 0.0004
