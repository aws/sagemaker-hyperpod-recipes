display_name: "Llama 3.2 1B Instruct Supervised Fine-Tuning Full Fine-Tuning on GPU"
version: "1.0.0"
instance_types: ['ml.p4d.24xlarge','ml.p4de.24xlarge','ml.p5.48xlarge']

run:
  name: llama-3-2-1b-instruct
  model_type: llm_finetuning_aws
  results_dir: ${base_results_dir}/${.name}
  hf_access_token: null

trainer:
  devices: 8
  num_nodes: 1

training_config:
  mlflow:
    tracking_uri: ""
    run_id: ""
  force_rerun: false
  model_config:
    model_name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
    lora_checkpoint_file_path: null
    multimodal: false
    peft_config: null
    model_save_name: SageMaker/llama-3.2-1B-Instruct-FFT
    chat_template_key: llama--meta-llama-3.2-1b-instruct
    apply_chat_template: false
    device_map: null
    attn_implementation: sdpa
    generation_cfg:
      max_new_tokens: 2048
      temperature: 0.6
      top_p: 0.9
      do_sample: true
      num_return_sequences: 1
      num_parallel_jobs: 1
  datasets:
    data_dir: null
    save_intermediate_processing_steps: false
    train_data:
      name: ""
      file_path: ""
      limit: 1000
    val_data:
      name: ""
      file_path: ""
      limit: null
    train_val_split_ratio: 0.9
    train_val_split_seed: 42
    preprocessor_cfgs:
    - type: TrainingSFTSampleConverter
    - type: Seq2SeqTokensProcessor
      chat_template_name: llama_3_2_seq_instruct.jinja
      ignore_prompt_labels: true
    - type: DropColumns
      column_names:
      - messages
  training_args:
    trainer_type: sft
    training_dir: ""
    override_training_dir: true
    seed: 42
    packing_samples: false
    ring_attn_size: 1
    ring_head_stride: 1
    pretrain_mode: false
    aux_loss_coef: 0
    learning_rate: 0.0001
    lr_warmup_ratio: 0.1
    weight_decay: 0.0
    adam_betas:
    - 0.9
    - 0.95
    lr_scheduler: cosine
    l2: 0.0
    gradient_clipping: true
    gradient_clipping_threshold: 1.0
    grad_accum_dtype: null
    gradient_checkpointing: true
    gradient_checkpointing_use_reentrant: false
    load_checkpoint: false
    resume_checkpoint_path: null
    disable_ds_ckpt: true
    disable_fast_tokenizer: false
    micro_train_batch_size: 1
    train_batch_size: 8
    eval_steps: 0
    save_steps: 0
    save_hf_ckpt: true
    merge_weights: true
    max_epochs: 5
    max_len: 4096
    max_norm: 1.0
    max_samples: 100000000.0
    max_ckpt_mem: 100000000.0
    max_ckpt_num: 3
    logging_steps: 1
    use_tensorboard: logs
    use_wandb: null
    strategy:
      fsdp_config:
        auto_wrap_policy: TRANSFORMER_BASED_WRAP
        backward_prefetch: BACKWARD_PRE
        cpu_ram_efficient_loading: true
        cpu_offload: false
        forward_prefetch: false
        sharding_strategy: FULL_SHARD
        state_dict_type: FULL_STATE_DICT
        sync_module_states: true
        use_orig_params: true
