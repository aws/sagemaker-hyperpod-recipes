defaults:
  - /hydra_config/llmft/default
  - /hydra_config/llmft/model_config/llama-3-2-11b-vision-instruct
  - /hydra_config/llmft/peft/lora
  - /hydra_config/llmft/algorithm/sft
  - /hydra_config/llmft/datasets/default_sft_vision
  - /hydra_config/llmft/strategy/fsdp_fft
  - _self_

display_name: "Llama 3.2 Vision 11B Supervised Fine-Tuning with LoRA on GPU, 4K sequence length"
version: "1.2.0"
instance_types: ['ml.p5.48xlarge','ml.p5en.48xlarge']

run:
  name: llama-3-2-11b-vision-instruct

training_config:
  force_rerun: true
  model_config:
    model_name_or_path: "meta-llama/Llama-3.2-11B-Vision-Instruct"
    model_save_name: Meta-Llama-3.2-11B-Instruct
    chat_template_key: llama--meta-llama-3.2-11b-instruct
    multimodal: true
    generation_cfg:
      temperature: 0
    peft_config:
      peft_type: lora
      target_modules: all-linear
      r: 16
      lora_alpha: 32
      lora_dropout: 0.05
      bias: none
      task_type: CAUSAL_LM
      use_dora: false
      load_in_4bit: false
      bf16: true
  datasets:
    preprocessor_cfgs: null
  training_args:
    train_batch_size: 16
    max_epochs: 1
    merge_weights: true
