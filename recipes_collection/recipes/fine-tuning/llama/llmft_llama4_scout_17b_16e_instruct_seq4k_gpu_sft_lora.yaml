defaults:
  - /hydra_config/llmft/default
  - /hydra_config/llmft/model_config/llama-4-scout-17b-16e-instruct
  - /hydra_config/llmft/peft/lora
  - /hydra_config/llmft/algorithm/sft
  - /hydra_config/llmft/datasets/default_sft
  - /hydra_config/llmft/strategy/fsdp_peft
  - _self_

display_name: "Llama 4 Scout 17B 16E Supervised Fine-Tuning with LoRA on GPU, 4K sequence length"
version: "1.1.0"
instance_types: ['ml.p5.48xlarge', 'ml.p5e.48xlarge']

run:
  name: llama-4-17b-16e-instruct

training_config:
  force_rerun: true
  model_config:
    model_name_or_path: "meta-llama/Llama-4-Scout-17B-16E-Instruct"
    model_save_name: Meta-Llama-4-Scout-17B-16E-Instruct
    peft_config:
      peft_type: lora
      target_modules: all-linear
      r: 16
      lora_alpha: 32
      lora_dropout: 0.05
      bias: none
      task_type: CAUSAL_LM
      use_dora: false
      exclude_modules:
        - model.layers.0.feed_forward.router
        - model.layers.1.feed_forward.router
        - model.layers.2.feed_forward.router
        - model.layers.3.feed_forward.router
        - model.layers.4.feed_forward.router
        - model.layers.5.feed_forward.router
        - model.layers.6.feed_forward.router
        - model.layers.7.feed_forward.router
        - model.layers.8.feed_forward.router
        - model.layers.9.feed_forward.router
        - model.layers.10.feed_forward.router
        - model.layers.11.feed_forward.router
        - model.layers.12.feed_forward.router
        - model.layers.13.feed_forward.router
        - model.layers.14.feed_forward.router
        - model.layers.15.feed_forward.router
        - model.layers.16.feed_forward.router
        - model.layers.17.feed_forward.router
        - model.layers.18.feed_forward.router
        - model.layers.19.feed_forward.router
        - model.layers.20.feed_forward.router
        - model.layers.21.feed_forward.router
        - model.layers.22.feed_forward.router
        - model.layers.23.feed_forward.router
        - model.layers.24.feed_forward.router
        - model.layers.25.feed_forward.router
        - model.layers.26.feed_forward.router
        - model.layers.27.feed_forward.router
        - model.layers.28.feed_forward.router
        - model.layers.29.feed_forward.router
        - model.layers.30.feed_forward.router
        - model.layers.31.feed_forward.router
        - model.layers.32.feed_forward.router
        - model.layers.33.feed_forward.router
        - model.layers.34.feed_forward.router
        - model.layers.35.feed_forward.router
        - model.layers.36.feed_forward.router
        - model.layers.37.feed_forward.router
        - model.layers.38.feed_forward.router
        - model.layers.39.feed_forward.router
        - model.layers.40.feed_forward.router
        - model.layers.41.feed_forward.router
        - model.layers.42.feed_forward.router
        - model.layers.43.feed_forward.router
        - model.layers.44.feed_forward.router
        - model.layers.45.feed_forward.router
        - model.layers.46.feed_forward.router
        - model.layers.47.feed_forward.router
  datasets:
    preprocessor_cfgs:
      - type: TrainingSFTSampleConverter
      - type: Seq2SeqTokensProcessor
        chat_template_name: llama_3_2_seq_instruct.jinja
        ignore_prompt_labels: true
      - type: DropColumns
        column_names:
          - messages
  training_args:
    train_batch_size: 16
    merge_weights: true

elastic_policy:
  is_elastic: false
  min_nodes: 1
  max_nodes: 16
  use_graceful_shutdown: true
  scaling_timeout: 600
  graceful_shutdown_timeout: 1200

scale_config:
  1:
    trainer:
      num_nodes: 1
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 4
        learning_rate: 0.0004
  2:
    trainer:
      num_nodes: 2
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 4
        learning_rate: 0.0004
  4:
    trainer:
      num_nodes: 4
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 4
        learning_rate: 0.0004
  6:
    trainer:
      num_nodes: 6
    training_config:
      training_args:
        train_batch_size: 128
        learning_rate: 0.0004
        uneven_batch:
          use_uneven_batch: true
          num_dp_groups_with_small_batch_size: 16
          small_local_batch_size: 2
          large_local_batch_size: 3
  8:
    trainer:
      num_nodes: 8
    training_config:
      training_args:
        train_batch_size: 128
        micro_train_batch_size: 2
        learning_rate: 0.0004
  16:
    trainer:
      num_nodes: 16
    training_config:
      training_args:
        train_batch_size: 128
        learning_rate: 0.0004
