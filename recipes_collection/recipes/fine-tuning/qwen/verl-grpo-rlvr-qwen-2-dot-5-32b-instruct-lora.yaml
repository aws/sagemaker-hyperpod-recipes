defaults:
  - /hydra_config/verl/default
  - /hydra_config/verl/algorithm/grpo
  - /hydra_config/verl/actor/default
  - /hydra_config/verl/ref/default
  - /hydra_config/verl/critic/default
  - /hydra_config/verl/rollout/vllm
  - /hydra_config/verl/actor_rollout_ref/default
  - /hydra_config/verl/trainer/default
  - /hydra_config/verl/data/default
  - /hydra_config/verl/reward_model/default
  - /hydra_config/verl/ray_cluster/default
  - /hydra_config/verl/peft/lora
  - /hydra_config/verl/reward/rlvr
  - /hydra_config/verl/model_config/qwen-2-5-32b-instruct
  - _self_

display_name: "Qwen 2.5 32B Instruct GRPO RLVR Fine-Tuning"
version: "1.1.0"
instance_types: ["ml.p5.48xlarge", "ml.p4de.24xlarge"]

run:
  name: verl-grpo-qwen-2-dot-5-32b-instruct-lora

trainer:
  num_nodes: 1

ray_cluster:
  worker_nodes:
    replicas: 1

training_config:
  actor_rollout_ref:
    actor:
      ppo_max_token_len_per_gpu: 20480
      optim:
        lr: 1.0e-05
    ref:
      log_prob_micro_batch_size_per_gpu: 16
      log_prob_max_token_len_per_gpu: 20480
    rollout:
      prompt_length: 512
      response_length: 1024
      tensor_model_parallel_size: 4
      log_prob_micro_batch_size_per_gpu: 16
      log_prob_max_token_len_per_gpu: 20480
      enable_chunked_prefill: true
      layered_summon: false
      merge_lora_for_inference: false
    model:
      is_saving_checkpoint_in_lora: true
  trainer:
    project_name: grpo_throughput_gsm8k
    experiment_name: Qwen2.5-32B_L-true_NN-1_TP-4_R-6
    logger:
    - console
    - mlflow
    resume_mode: resume_path
    resume_from_path: ''
    test_freq: 7
  data:
    max_prompt_length: 512
    max_response_length: 1024
    val_batch_size: 256
  critic:
    rollout_n: 6
    use_dynamic_bsz: true
    ppo_max_token_len_per_gpu: 20480
  reward_model:
    use_dynamic_bsz: true
    reward_manager: prime
    model:
      path: sfairX/FsfairX-LLaMA3-RM-v0.1
  custom_reward_function:
    lambda_arn: ''
    path: ''
    name: ''
