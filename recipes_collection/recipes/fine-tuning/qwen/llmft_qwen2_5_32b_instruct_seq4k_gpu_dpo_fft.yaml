defaults:
  - /hydra_config/llmft/default
  - /hydra_config/llmft/model_config/qwen-2-5-32b-instruct
  - /hydra_config/llmft/peft/fft
  - /hydra_config/llmft/algorithm/dpo
  - /hydra_config/llmft/datasets/default_dpo_fft
  - /hydra_config/llmft/strategy/fsdp_dpo_fft
  - _self_

display_name: "Qwen 2.5 32B Instruct Direct Preference Optimization Full Fine-Tuning on GPU"
version: "1.2.0"
instance_types: ['ml.p5.48xlarge']

run:
  name: qwen-2d5-32b

training_config:
  model_config:
    model_name_or_path: "Qwen/Qwen2.5-32B-Instruct"
    model_save_name: SageMaker-FFT/Qwen2.5-32B-Instruct
  training_args:
    train_batch_size: 32
    gradient_checkpointing_use_reentrant: true
    ulysses_sequence_parallel_size: 1
    log_run_time: false
    log_gpu_memory: false
    strategy:
      fsdp_config:
        cpu_offload: true
        activation_checkpointing: true
