cluster_type: k8s
debug: false
dry_run: false
launch_json: true
instance_type: p5.48xlarge
base_results_dir: '{{results_directory}}'
container: '{{container_image}}'
git:
  repo_url_or_path: null
  branch: null
  commit: null
  entry_script: /app/src/train_hp.py
  token: null
  update_adapter: false
  use_default: false
env_vars:
  NCCL_DEBUG: WARN
training_config: fine-tuning/llama/llmft_llama3_2_1b_instruct_seq4k_gpu_sft_lora
cluster:
  pullPolicy: Always
  restartPolicy: Never
  namespace: default
  custom_labels: null
  annotations: null
  service_account_name: null
  priority_class_name: null
  volumes: null
  persistent_volume_claims:
  - claimName: fsx-claim
    mountPath: /data
  label_selector: null
  cleanPodPolicy: null
recipes:
  run:
    name: '{{job_name}}'
    model_type: llm_finetuning_aws
    results_dir: '{{results_directory}}'
    hf_access_token: null
  trainer:
    devices: 8
    num_nodes: 1
  displayName: Llama 3.2 1B Simple Fine Tuning with Lora on GPU, 4K sequence length
  versions:
  - '1.0'
  training_config:
    force_rerun: false
    model_config:
      model_name_or_path: meta-llama/Llama-3.2-1B-Instruct
      lora_checkpoint_file_path: null
      multimodal: false
      peft_config:
        peft_type: lora
        target_modules: all-linear
        r: 16
        lora_alpha: '{{alpha}}'
        lora_dropout: 0.05
        bias: none
        task_type: CAUSAL_LM
        use_dora: false
      model_save_name: SageMaker/llama-3.2-1B-Instruct-Lora
      chat_template_key: llama--meta-llama-3.2-1b-instruct
      apply_chat_template: false
      device_map: null
      attn_implementation: sdpa
      generation_cfg:
        max_new_tokens: '{{max_response_length}}'
        temperature: '{{temperature}}'
        top_p: '{{top_p}}'
        do_sample: true
        num_return_sequences: 1
        num_parallel_jobs: 1
    datasets:
      data_dir: null
      save_intermediate_processing_steps: false
      train_data:
        name: '{{training_data_name}}'
        file_path: '{{training_data_file_path}}'
        limit: null
      val_data:
        name: '{{validation_data_name}}'
        file_path: '{{validation_data_file_path}}'
        limit: null
      train_val_split_ratio: 0.9
      train_val_split_seed: 42
      preprocessor_cfgs:
      - type: TrainingSFTSampleConverter
      - type: Seq2SeqTokensProcessor
        chat_template_name: llama_3_2_seq_instruct.jinja
        ignore_prompt_labels: true
      - type: DropColumns
        column_names:
        - messages
    training_args:
      trainer_type: sft
      training_dir: ''
      seed: 42
      packing_samples: false
      ring_attn_size: 1
      ring_head_stride: 1
      pretrain_mode: false
      aux_loss_coef: 0
      learning_rate: '{{learning_rate}}'
      lr_warmup_ratio: '{{lr_warmup_ratio}}'
      weight_decay: 0.0
      adam_betas:
      - 0.9
      - 0.95
      lr_scheduler: cosine
      l2: 0.0
      gradient_clipping: true
      gradient_clipping_threshold: 1.0
      grad_accum_dtype: null
      gradient_checkpointing: true
      gradient_checkpointing_use_reentrant: false
      load_checkpoint: false
      resume_checkpoint_path: null
      disable_ds_ckpt: true
      disable_fast_tokenizer: false
      micro_train_batch_size: '{{micro_train_batch_size}}'
      train_batch_size: '{{train_batch_size}}'
      eval_steps: 0
      save_steps: 0
      save_hf_ckpt: true
      merge_weights: true
      max_epochs: '{{max_epochs}}'
      max_len: 4096
      max_norm: 1.0
      max_samples: 100000000.0
      max_ckpt_mem: 100000000.0
      max_ckpt_num: 3
      logging_steps: 1
      use_tensorboard: logs
      use_wandb: null
      strategy:
        fsdp_config:
          auto_wrap_policy: TRANSFORMER_BASED_WRAP
          backward_prefetch: BACKWARD_PRE
          cpu_ram_efficient_loading: true
          cpu_offload: false
          forward_prefetch: false
          sharding_strategy: FULL_SHARD
          state_dict_type: FULL_STATE_DICT
          sync_module_states: true
          use_orig_params: true
launcher_scripts_path: ./launcher/nemo/nemo_framework_launcher/launcher_scripts/
wandb_api_key_file: null
wandb_api_bcp_secret_key: null
