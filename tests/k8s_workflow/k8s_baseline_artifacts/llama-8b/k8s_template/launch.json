{
  "training-config.yaml": "---\n# Source: sagemaker-training/templates/training-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: training-config-llama-8b\ndata:\n  config.yaml: |-\n      llama-8b_hydra.yaml: |\n      run:\n        name: llama-8b\n        results_dir: /tmp/test_recipe_k8s_workflow_with_launch_json/llama-8b\n        time_limit: 6-00:00:00\n        model_type: hf\n      trainer:\n        devices: 8\n        num_nodes: 16\n        accelerator: gpu\n        precision: bf16\n        max_steps: 50\n        log_every_n_steps: 1\n        val_check_interval: 1\n        limit_val_batches: 0\n      exp_manager:\n        exp_dir: null\n        name: experiment\n        create_tensorboard_logger: false\n        summary_writer_kwargs:\n          save_dir: None/tensorboard\n        create_mlflow_logger: false\n        mlflow_logger_kwargs:\n          tracking_uri: None/mlflow\n        create_wandb_logger: false\n        wandb_logger_kwargs:\n          save_dir: null\n        create_checkpoint_callback: true\n        checkpoint_callback_params:\n          save_top_k: 0\n          every_n_train_steps: 10\n          monitor: step\n          mode: max\n          save_last: false\n        checkpoint_dir: None/checkpoints/\n        resume_from_checkpoint: null\n        auto_checkpoint:\n          enabled: false\n        export_full_model:\n          every_n_train_steps: 0\n          save_last: true\n      use_smp_model: true\n      distributed_backend: nccl\n      model:\n        model_type: llama_v3\n        train_batch_size: 1\n        val_batch_size: 1\n        seed: 12345\n        grad_clip: 1.0\n        log_reduced_training_loss: true\n        tensor_model_parallel_degree: 2\n        expert_model_parallel_degree: 1\n        context_parallel_degree: 1\n        moe: false\n        activation_checkpointing: false\n        activation_loading_horizon: 1\n        delayed_param: true\n        offload_activations: false\n        sharding_strategy: hybrid_shard\n        forward_prefetch: true\n        shard_degree: 64\n        backward_fetch_policy: backward_pre\n        auto_wrap_policy: transformer_auto_wrap_policy\n        limit_all_gathers: true\n        use_orig_param: true\n        fp8: true\n        fp8_amax_history_len: 1024\n        fp8_amax_compute_algo: max\n        max_context_width: 16384\n        max_position_embeddings: 16384\n        num_hidden_layers: 32\n        hidden_size: 4096\n        num_attention_heads: 32\n        intermediate_size: 14336\n        initializer_range: 0.02\n        layernorm_epsilon: 1.0e-05\n        vocab_size: 128256\n        num_key_value_heads: 8\n        use_flash_attention: true\n        rope_theta: 500000.0\n        rope_scaling:\n          rope_type: llama3\n          factor: 8.0\n          high_freq_factor: 4.0\n          low_freq_factor: 1.0\n          original_max_position_embeddings: 8192\n        do_finetune: false\n        hf_model_name_or_path: null\n        peft:\n          peft_type: null\n        precision: bf16\n        lr_decay_iters: 50\n        optim:\n          name: adamw\n          lr: 0.0001\n          weight_decay: 0.01\n          betas:\n          - 0.9\n          - 0.95\n          sched:\n            name: CosineAnnealing\n            warmup_steps: 0\n            constant_steps: 0\n            min_lr: 1.0e-06\n        data:\n          train_dir: null\n          val_dir: null\n          dataset_type: hf\n          use_synthetic_data: false\n        viztracer:\n          enabled: false\n",
  "training.yaml": "---\n# Source: sagemaker-training/templates/training.yaml\napiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  name: llama-8b\n  namespace: default\n  labels:\n    app: llama-8b\nspec:\n  pytorchReplicaSpecs:\n    Worker:\n      replicas: 16\n      restartPolicy: Never\n      template:\n        spec:\n          containers:\n          - name: pytorch\n            image: test_container\n            env:\n              - name: CUDA_DEVICE_MAX_CONNECTIONS\n                value: \"1\"\n              - name: CUDA_VISIBLE_DEVICES\n                value: \"0,1,2,3,4,5,6,7\"\n              - name: FI_PROVIDER\n                value: \"efa\"\n              - name: NCCL_DEBUG\n                value: \"WARN\"\n              - name: NCCL_IGNORE_DISABLED_P2P\n                value: \"1\"\n              - name: NCCL_SOCKET_IFNAME\n                value: \"^lo,docker0,veth_def_agent\"\n              - name: NEMO_LAUNCHER_DEBUG\n                value: \"1\"\n              - name: SLURM_NTASKS_PER_NODE\n                value: \"8\"\n              - name: TORCH_DIST_INIT_BARRIER\n                value: \"1\"\n              - name: TORCH_NCCL_ASYNC_ERROR_HANDLING\n                value: \"1\"\n              # New environmental variables for the script\n              - name: GIT_REPO_URL\n                value: \"https://test_token@github.com/aws/sagemaker-hyperpod-training-adapter-for-nemo.git\"\n              - name: GIT_BRANCH\n                value: \"test_branch\"\n              - name: GIT_COMMIT\n                value: \"test_commit\"\n              - name: GIT_UPDATE_ADAPTER\n                value: \"false\"\n              - name: NODES\n                value: \"16\"\n              - name: NTASKS_PER_NODE\n                value: \"8\"\n              - name: JOB_NAME\n                value: \"llama-8b\"\n              - name: SCRIPT_PATH\n                value: \"examples/llama/llama_pretrain.py\"\n              - name: SCRIPT_ARGS\n                value: \"--config-path=/config --config-name=config.yaml\"\n              - name: PRE_SCRIPT_COMMANDS\n                value: \"\"\n              - name: POST_SCRIPT_COMMANDS\n                value: \"\"\n            command:\n            - /bin/bash\n            - -c\n            - \"/opt/bin/scripts/train-script-gpu.sh 2>&1 | tee /data/training.log\"\n            imagePullPolicy: Always\n            securityContext:\n              privileged: true\n            resources:\n              requests:\n                nvidia.com/gpu: 8\n                vpc.amazonaws.com/efa: 32\n              limits:\n                nvidia.com/gpu: 8\n                vpc.amazonaws.com/efa: 32\n            volumeMounts:\n            - mountPath: /data\n              name: fsx-claim-volume\n            - mountPath: /config\n              name: training-config\n            - mountPath: /dev/shm\n              name: shm\n            - mountPath: /var/log/aws/clusters\n              name: aws-clusters-logs\n              readOnly: true\n\n          volumes:\n          - name: fsx-claim-volume\n            persistentVolumeClaim:\n              claimName: fsx-claim\n          - configMap:\n              name: training-config-llama-8b\n            name: training-config\n          - name: shm\n            hostPath:\n              path: /dev/shm\n              type: Directory\n          - name: aws-clusters-logs\n            hostPath:\n              path: /var/log/aws/clusters\n              type: DirectoryOrCreate\n"
}
