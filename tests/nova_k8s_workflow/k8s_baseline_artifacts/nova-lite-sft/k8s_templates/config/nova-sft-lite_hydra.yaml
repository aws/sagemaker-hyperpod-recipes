run:
  name: nova-sft-lite
  model_type: nova
  model_name_or_path: amazon/nova-lite-v1
  replicas: 2
training_config:
  max_length: 32768
  save_steps: 100000
  replicas: 2
  micro_batch_size: 1
  global_batch_size: 64
  weights_only: true
  allow_percentage_invalid_samples: 10
  exp_manager:
    exp_dir: null
    create_wandb_logger: false
    create_tensorboard_logger: true
    summary_writer_kwargs:
      save_dir: None/tensorboard
    create_mlflow_logger: false
    mlflow_logger_kwargs:
      tracking_uri: None/mlflow
    train_data_s3_path: s3://inference-test-nova/train.jsonl
    validation_data_s3_path: ''
    wandb_logger_kwargs:
      project: null
      name: null
    checkpoint_callback_params:
      monitor: step
      save_top_k: 10
      mode: max
      every_n_train_steps: 100000
      save_last: true
    create_early_stopping_callback: true
    early_stopping_callback_params:
      min_delta: 0.001
      mode: min
      monitor: val_loss
      patience: 2
  trainer:
    log_every_n_steps: 1
    max_epochs: -1
    val_check_interval: 100
    limit_test_batches: 0
    gradient_clip_val: 1.0
    num_nodes: 2
  model:
    hidden_dropout: 0.0
    attention_dropout: 0.0
    ffn_dropout: 0.0
    sequence_parallel: true
    optim:
      lr: 1.0e-05
      name: distributed_fused_adam
      bucket_cap_mb: 10
      contiguous_grad_buffer: false
      overlap_param_sync: false
      contiguous_param_buffer: false
      overlap_grad_sync: false
      adam_w_mode: true
      eps: 1.0e-06
      weight_decay: 0.0
      betas:
      - 0.9
      - 0.999
      sched:
        name: CosineAnnealing
        warmup_steps: 10
        constant_steps: 0
        min_lr: 1.0e-06
    mm_cfg:
      llm:
        freeze: true
      supported_modalities:
      - image
      - video
      image_projector:
        freeze: true
        require_newline: true
      video_projector:
        freeze: true
        require_newline: false
    peft:
      peft_scheme: lora
      lora_tuning:
        target_modules:
        - attention_qkv
        - mlp_fc1
        - mlp_fc2
        - attention_dense
        loraplus_lr_ratio: 8.0
        adapter_dim: 32
        alpha: 32
        adapter_dropout: 0.01
    training_validation:
      loader:
        args:
          data_loader_workers: 1
          prefetch_factor: 2
      collator:
        args:
          force_image_at_turn_beginning: false
