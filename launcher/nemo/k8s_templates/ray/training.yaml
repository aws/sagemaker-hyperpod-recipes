apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: {{ .Values.trainingConfig.jobName }}
  namespace: {{ .Values.trainingConfig.namespace }}
spec:
  shutdownAfterJobFinishes: {{ .Values.gc.shutdownAfterJobFinishes | default true }}
  ttlSecondsAfterFinished: {{ .Values.gc.ttlSecondsAfterFinished | default 600 }}

  # Ensure submitter is a K8s Job so we can schedule it
  submissionMode: K8sJobMode
  submitterPodTemplate:
    spec:
      restartPolicy: Never
      serviceAccountName: {{ .Values.trainingConfig.serviceAccountName | default "default" }}
      {{- if .Values.trainingConfig.cpuInstanceTypes }}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      {{- range .Values.trainingConfig.cpuInstanceTypes }}
                      - {{ . | quote }}
                      {{- end }}
      {{- end }}
      containers:
        - name: ray-job-submitter
          image: {{ .Values.image.trainingImage | quote }}
          imagePullPolicy: {{ .Values.image.pullPolicy | default "IfNotPresent" }}
          resources:
            requests:
              cpu: {{ .Values.rayCluster.jobSubmitter.cpu | quote }}
              memory: {{ .Values.rayCluster.jobSubmitter.memory | quote }}
            {{- if or .Values.rayCluster.jobSubmitter.cpuLimit .Values.rayCluster.jobSubmitter.memoryLimit }}
            limits:
              {{- if .Values.rayCluster.jobSubmitter.cpuLimit }}
              cpu: {{ .Values.rayCluster.jobSubmitter.cpuLimit | quote }}
              {{- end }}
              {{- if .Values.rayCluster.jobSubmitter.memoryLimit }}
              memory: {{ .Values.rayCluster.jobSubmitter.memoryLimit | quote }}
              {{- end }}
            {{- end }}

  entrypoint: >-
    python -u -m verl.trainer.main_ppo --config-path /config --config-name verl_config

  rayClusterSpec:
    rayVersion: {{ .Values.rayCluster.rayVersion | quote }}

    headGroupSpec:
      rayStartParams:
        dashboard-host: "0.0.0.0"
        dashboard-port: "8265"
      template:
        metadata:
          labels:
            ray-job-name: {{ .Values.trainingConfig.jobName | quote }}
            ray-role: "head"
            ray-co-locate: "true"
        spec:
          serviceAccountName: {{ .Values.trainingConfig.serviceAccountName | default "default" }}

          # head must be scheduled onto GPU worker instance type nodes, to have workers scheduled on the same node
          {{- if .Values.trainingConfig.workerInstanceType }}
          nodeSelector:
            node.kubernetes.io/instance-type: {{ .Values.trainingConfig.workerInstanceType | quote }}
          {{- end }}

          containers:
          - name: ray-head
            image: {{ .Values.image.trainingImage | quote }}
            imagePullPolicy: {{ .Values.image.pullPolicy | default "IfNotPresent" }}
            env:
            - name: MLFLOW_TRACKING_URI
              value: ""
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: TRAIN_FILES
              value: {{ .Values.trainingConfig.trainDir | quote }}
            - name: VAL_FILES
              value: {{ .Values.trainingConfig.valDir | quote }}
            volumeMounts:
            - name: training-config
              mountPath: /config
              readOnly: true
            {{- if .Values.trainingConfig.persistentVolumeClaims }}
            {{- range .Values.trainingConfig.persistentVolumeClaims }}
            {{- if . }}
            - name: {{ .claimName }}
              mountPath: /{{ .mountPath }}
            {{- end }}
            {{- end }}
            {{- end }}
            resources:
              requests:
                cpu: {{ .Values.rayCluster.headNode.cpu | quote }}
                memory: {{ .Values.rayCluster.headNode.memory | quote }}
                nvidia.com/gpu: {{ .Values.rayCluster.headNode.gpu | quote }}
              {{- $cpulim := .Values.rayCluster.headNode.cpuLimit }}
              {{- $memlim := .Values.rayCluster.headNode.memoryLimit }}
              {{- $gpucnt := .Values.rayCluster.headNode.gpu }}
              {{- if or $cpulim $memlim $gpucnt }}
              limits:
                {{- if $cpulim }}
                cpu: {{ $cpulim | quote }}
                {{- end }}
                {{- if $memlim }}
                memory: {{ $memlim | quote }}
                {{- end }}
                {{- if $gpucnt }}
                nvidia.com/gpu: {{ $gpucnt | quote }}
                {{- end }}
              {{- end }}

          volumes:
          - name: training-config
            configMap:
              name: training-config-{{ .Values.trainingConfig.jobName }}
              defaultMode: 0755
          {{- if .Values.trainingConfig.persistentVolumeClaims }}
          {{- range .Values.trainingConfig.persistentVolumeClaims }}
          {{- if . }}
          - name: {{ .claimName }}
            persistentVolumeClaim:
              claimName: {{ .claimName }}
          {{- end }}
          {{- end }}
          {{- end }}

    workerGroupSpecs:
    - groupName: workers
      replicas: {{ .Values.rayCluster.workerNodes.replicas }}
      rayStartParams: {}
      template:
        metadata:
          labels:
            ray-job-name: {{ .Values.trainingConfig.jobName | quote }}
            ray-role: "worker"
            ray-co-locate: "true"
        spec:
          serviceAccountName: {{ .Values.trainingConfig.serviceAccountName | default "default" }}

          # Keep existing behavior: pin workers to GPU instance type
          {{- if .Values.trainingConfig.workerInstanceType }}
          nodeSelector:
            node.kubernetes.io/instance-type: {{ .Values.trainingConfig.workerInstanceType | quote }}
          {{- end }}

          # workers tries to be on the same node as worker pod
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    topologyKey: kubernetes.io/hostname
                    labelSelector:
                      matchExpressions:
                        - key: ray-job-name
                          operator: In
                          values:
                            - {{ .Values.trainingConfig.jobName | quote }}
                        - key: ray-role
                          operator: In
                          values:
                            - "head"
                        - key: ray-co-locate
                          operator: In
                          values:
                            - "true"

          containers:
          - name: ray-worker
            image: {{ .Values.image.trainingImage | quote }}
            imagePullPolicy: {{ .Values.image.pullPolicy | default "IfNotPresent" }}
            env:
            - name: MLFLOW_TRACKING_URI
              value: ""
            volumeMounts:
            {{- if .Values.trainingConfig.persistentVolumeClaims }}
            {{- range .Values.trainingConfig.persistentVolumeClaims }}
            {{- if . }}
            - name: {{ .claimName }}
              mountPath: /{{ .mountPath }}
            {{- end }}
            {{- end }}
            {{- end }}
            resources:
              requests:
                cpu: {{ .Values.rayCluster.workerNodes.cpu | quote }}
                memory: {{ .Values.rayCluster.workerNodes.memory | quote }}
                nvidia.com/gpu: {{ .Values.rayCluster.workerNodes.gpu | quote }}
              {{- $cpulim := .Values.rayCluster.workerNodes.cpuLimit }}
              {{- $memlim := .Values.rayCluster.workerNodes.memoryLimit }}
              {{- $gpucnt := .Values.rayCluster.workerNodes.gpu }}
              {{- if or $cpulim $memlim $gpucnt }}
              limits:
                {{- if $cpulim }}
                cpu: {{ $cpulim | quote }}
                {{- end }}
                {{- if $memlim }}
                memory: {{ $memlim | quote }}
                {{- end }}
                {{- if $gpucnt }}
                nvidia.com/gpu: {{ $gpucnt | quote }}
                {{- end }}
              {{- end }}

          volumes:
          {{- if .Values.trainingConfig.persistentVolumeClaims }}
          {{- range .Values.trainingConfig.persistentVolumeClaims }}
          {{- if . }}
          - name: {{ .claimName }}
            persistentVolumeClaim:
              claimName: {{ .claimName }}
          {{- end }}
          {{- end }}
          {{- end }}
